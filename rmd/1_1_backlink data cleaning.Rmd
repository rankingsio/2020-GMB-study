---
title: "1_1_backlink data cleaning"
output: html_document
---
```{r}
required_packages <- c("tidyverse", "readxl", "ggthemes", "hrbrthemes", "extrafont", "plotly", "scales", "stringr", "gganimate", "here", "tidytext", "sentimentr", "scales", "DT", "here", "sm", "mblm", "prettydoc", "reshape2", "treemapify", "glue", "magick", "imager", "fs", "knitr", "DataExplorer", "inspectdf", "rmdformats", "prettydoc", "janitor", "urltools")

for(i in required_packages) { 
if(!require(i, character.only = T)) {
#  if package is not existing, install then load the package
install.packages(i, dependencies = T)
require(i, character.only = T)
}
}
```



```{r}
basic <- read_csv(here("raw_data", "GMB data", "basic.csv"))

urls_for_backlinks <- basic %>% select(place_id, website) %>% 
  distinct(place_id, .keep_all = T) %>% 
  drop_na(website)
# leads to same number of rows as the data from Anastasia. 



#clean domain names to the root
library(urltools)

urls_for_backlinks <- urls_for_backlinks %>% mutate(clean_domain_names = suffix_extract(domain(website)))  
     
#unnest domain names into one tibble

urls_for_backlinks <- do.call(data.frame, urls_for_backlinks) %>%  #reduce(data.frame) works also, see https://stackoverflow.com/questions/62313519/unnest-dataframe-within-dataframe-r 
  as_tibble() %>% 
  select(place_id, website, clean_domain_names.host) %>% 
  rename(clean_domain_names = clean_domain_names.host)
  
```


```{r}
urls_for_backlinks %>% count(clean_domain_names, sort = T) %>% View()
#some google domains, will filter them out

#filter out google domains
urls_for_backlinks <- urls_for_backlinks %>% 
  filter(!str_detect(clean_domain_names, "google")) 

#save file in proc_data
#write_csv(urls_for_backlinks, here("proc_data", "1_1_urls_for_backlinks_clean_domain_names.csv"))


```


```{r}
#save file in proc data unique domains and give VA to plug into ahrefs
n_distinct(urls_for_backlinks$clean_domain_names)

unique_urls <- urls_for_backlinks %>% distinct(urls_for_backlinks$clean_domain_names) %>% 
   rename(website = `urls_for_backlinks$clean_domain_names`)

#write_csv(unique_urls, here("proc_data", "1_1_unique_urls_for_VA.csv"))
```


```{r}
library(googlesheets4)

urls_with_ahrefs_data <- read_sheet("https://docs.google.com/spreadsheets/d/1DWBncf6HfgptVtVHV_Mu9vgJI17-MpFNdzVxXmNnqPs/edit#gid=596870066", sheet = "backlink_data")


# check which domains to remove
final_websites_with_backlinks %>% count(domain_rating) 
final_websites_with_backlinks %>% count(ref_domains_dofollow) %>% View() 

#business.site websites

final_websites_with_backlinks %>% filter(str_detect(clean_domain_names, "business.site")) %>% 
  slice_sample(n = 5) %>% 
  select(clean_domain_names, domain_rating, ref_domains_total, ref_domains_dofollow) %>% View()
# [1] 360

# websites to filter out
strings_websites_to_remove <- final_websites_with_backlinks %>% 
  filter(final_websites_with_backlinks$domain_rating > 85) %>%
  select(clean_domain_names, website) %>%
  filter(str_detect(clean_domain_names, "business.site")) %>% #keep business.site websites
  count(clean_domain_names, sort = T) %>% 
  select(clean_domain_names) %>% 
  unlist()

strings_websites_to_remove  

#merge backlink data with urls (not unique urls!)
final_websites_with_backlinks <- urls_for_backlinks %>% 
    left_join(urls_with_ahrefs_data, by = c("clean_domain_names" = "Target")) %>%
  clean_names() %>% 
  rename(ref_domains_total = domains) %>% 
  filter(!str_detect(clean_domain_names, paste(strings_websites_to_remove, collapse = "|")))   # remove domains with high DR



```

