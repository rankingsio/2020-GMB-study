---
title: "2_analysis"
output: html_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r results=FALSE, message=FALSE, echo=FALSE}
  #load required packages, if package is not found try to install
  required_packages <- c("xgboost", "data.table", "stringr", "SHAPforxgboost", "ggplot2")

  not_installed_packages <- required_packages[!(required_packages %in% installed.packages()[,"Package"])]
  if(length(not_installed_packages) > 0) 
    install.packages(not_installed_packages)
  was_succesfully_loaded <- sapply(required_packages, require, character.only = TRUE)
  
  if(any(!was_succesfully_loaded)) {
    cat(required_packages[!was_succesfully_loaded],"\n")
  }
    

```

## To-Do-List
* Add/improve explanations and interpretation
* More attributes/data to the model
* Clean up code

## Data

Google My Business search results were collected for four unique keyword combinations in 426 US cities, giving in total 1659 results. The format of the search queries was one the following:
* (city) + " car accident lawyer"  
* (city) + " personal injury lawyer" 
* (city) + " car accident attorney"
* (city) + " personal injury attorney"

#### Independent variables
All variables with prefix "Relative" are calculated as rank(values) / length(values), for values in each search group. E.g. for search with keyword "Milwaukee car accident lawyer" the entry with highest number of photos would get Relative number of photos value of 1, entry with lowest value would get 0, and the rest of the values would be something in between. The motivation behind this transformation is to make attribute values more comparable between searches, i.e. try to eliminate effect of the size of the population of the city etc.

* Attributes from basic.csv
  + Relative rating
  + Relative reviews count
  + Relative photos count
  + Relative questions count

  + Google Ads
  + Provides updates
  + Is business claimed

  + Has street address
  + Has website
  + Has phone

  + Keywords in description count
  + Keywords in title count
  + Has city in description
  + Has city in title 
  + Has lawyer or attorney in title
  + Has lawyer or attorney in description
  + Has car in description
  + Has car in title 
  + Has injury in description
  + Has injury in title
  + Character count in description 
  + Character count in title 

  + Type category personal injury attorney 
  + Type category lawyer 
  + Type category law firm
  + Type category empty
  + Type category criminal justice attorney
  + Type category other 

* Attributes from social_profile.csv
  + Has twitter
  + Has fb
  + Has youtube
  + Has instagram
  + Has linkedin
   
* Attributes from all_reviews.csv
  + Ratio of 5s in ratings 
  + Ratio of 4s in ratings
  + Ratio of 3s in ratings
  + Ratio of 2s in ratings
  + Ratio of 1s in ratings
  + Review response ratio
  + Relative review likes count
  
* Attributes from q_a.csv
  + Question answer ratio
  
* Attributes from backlink_data_websites.csv
  + Relative domain_rating
  + Relative ahrefs_rank
  + Relative ref_domains_total
  + Relative ref_domains_dofollow
  + Relative linked_domains
  + Relative total_backlinks
  + Relative backlinks_no_follow
  + Relative total_keywords
  + Relative total_traffic

#### Dependent variables
* Position in the search results

```{r data, cache=TRUE, echo = FALSE}

#read csv
basic <- fread("raw_data/GMB data/basic.csv")
social_profile <- fread("raw_data/GMB data/social_profile.csv")
user_review_featured <- fread("raw_data/GMB data/user_review_featured.csv")
q_a <- fread("raw_data/GMB data/q_a.csv")
all_reviews <- fread("raw_data/GMB data/all_reviews.csv")
backlink_data_websites <- fread("raw_data/GMB data/backlink_data_websites.csv")
cities <- fread("raw_data/uscities_raw.csv")
most_relevant_reviews <- fread("raw_data/GMB data/most_relevant_reviews.csv")

#keep only the latest scraped rows for social_profile
social_profile[, scraping_date := as.Date(scraping_date)]
social_profile <- social_profile[, .SD[which.max(scraping_date)], by="place_id"]

#process data for modeling, create/transform attributes etc
data_all <- merge(basic, social_profile[, c("place_id","twitter","fb","youtube","instagram","linkedin"),with=F], by="place_id", all.x=T)
data_all[, target := (rank(-position, ties.method="average") - 1) / (.N-1), by=keyword_strings]

#calculate search group sizes and remove searches with high enough result
data_all[, group_size := .N, by=keyword_strings]
data_all <- data_all[group_size >= 10]

#relative variables, ie. normalize by dividing by median value of the whole search group
data_all[, 'Relative review rating' := rank(rating, na.last="keep") / sum(!is.na(rating)), by=keyword_strings]
data_all[, 'Relative reviews count' := rank(reviews, na.last="keep") / sum(!is.na(reviews)), by=keyword_strings]
data_all[, 'Relative photos count' := rank(number_of_photos, na.last="keep") / sum(!is.na(number_of_photos)), by=keyword_strings]
data_all[, 'Relative questions count' := rank(total_questions, na.last="keep") / sum(!is.na(total_questions)), by=keyword_strings]

#transform Yes/No to 1/0
data_all[, 'Google Ads' := as.integer(google_ad == "Yes")]
data_all[, 'Provides updates' := as.integer(provides_updates == "Yes")]
data_all[, 'Is business claimed' := as.integer(business_claimed == "Yes")]

#does attribute have content or is it empty)
data_all[, 'Has street address' := as.integer(street_address != "")]
data_all[, 'Has website' := as.integer(website != "")]
data_all[, 'Has phone' := as.integer(phone != "")]
data_all[, 'Has twitter' := as.integer(twitter != "")]
data_all[, 'Has fb' := as.integer(fb != "")]
data_all[, 'Has youtube' := as.integer(youtube != "")]
data_all[, 'Has instagram' := as.integer(instagram != "")]
data_all[, 'Has linkedin' := as.integer(linkedin != "")]

#description & title
data_all[, 'Keywords in description count' := sum(str_count(description, unlist(str_split(keyword, " ")))) , by = row.names(data_all)]
data_all[, 'Keywords in title count' := sum(str_count(title, unlist(str_split(keyword, " ")))) , by = row.names(data_all)]
data_all[, 'Has city in description' := as.integer(grepl(city, description, ignore.case = TRUE)), by = row.names(data_all)]
data_all[, 'Has city in title' := as.integer(grepl(city, title, ignore.case = TRUE)), by = row.names(data_all)]
data_all[, 'Has lawyer or attorney in title' := as.integer(grepl("lawyer|attorney", title, ignore.case = TRUE)), by = row.names(data_all)]
data_all[, 'Has lawyer or attorney in description' := as.integer(grepl("lawyer|attorney", description, ignore.case = TRUE)), by = row.names(data_all)]
data_all[, 'Has car in description' := as.integer(grepl("car", description, ignore.case = TRUE)), by = row.names(data_all)]
data_all[, 'Has car in title' := as.integer(grepl("car", title, ignore.case = TRUE)), by = row.names(data_all)]
data_all[, 'Has injury in description' := as.integer(grepl("injury", description, ignore.case = TRUE)), by = row.names(data_all)]
data_all[, 'Has injury in title' := as.integer(grepl("injury", title, ignore.case = TRUE)), by = row.names(data_all)]
data_all[, 'Character count in description' := nchar(description)]
data_all[, 'Character count in title' := nchar(title)]

#type_category features
common_types <- c("Personal injury attorney",
                  "Lawyer",
                  "Law firm",
                  "",
                  "Criminal justice attorney" 
)
data_all[, 'Type category personal injury attorney' := as.numeric(type_category == common_types[1])]
data_all[, 'Type category lawyer' := as.numeric(type_category == common_types[2])]
data_all[, 'Type category law firm' := as.numeric(type_category == common_types[3])]
data_all[, 'Type category empty' := as.numeric(type_category == common_types[4])]
data_all[, 'Type category criminal justice attorney' := as.numeric(type_category == common_types[5])]
data_all[, 'Type category other' := as.numeric(!(type_category %in% common_types))]

data_all[, 'Has same city listed as in search query' := as.integer(city == city_place)]

#features from user_review_featured
data_all[, 'Has featured reviews' := as.integer(place_id %in% unique(user_review_featured$place_id))]

#features from all_reviews
all_reviews[, time_since_review := 0]
all_reviews[grepl("minute", all_reviews_date), time_since_review := 1]
all_reviews[grepl("hour", all_reviews_date), time_since_review := 2]
all_reviews[grepl("day", all_reviews_date), time_since_review := 3]
all_reviews[grepl("week", all_reviews_date), time_since_review := 4]
all_reviews[grepl("month", all_reviews_date), time_since_review := 5]
all_reviews[grepl("year", all_reviews_date), time_since_review := 6]

data_all <- merge(data_all, by="place_id", all.x=T, all.y=F,
      all_reviews[, list(
        'Ratio of 5s in ratings' = sum(all_reviews_rating == 5) / .N,
        'Ratio of 4s in ratings' = sum(all_reviews_rating == 4) / .N,
        'Ratio of 3s in ratings' = sum(all_reviews_rating == 3) / .N,
        'Ratio of 2s in ratings' = sum(all_reviews_rating == 2) / .N,
        'Ratio of 1s in ratings' = sum(all_reviews_rating == 1) / .N,
        'Review response ratio' = sum(all_reviews_response_from_owner != "") / .N,
        'review_median_of_likes' = as.numeric(median(all_reviews_number_of_likes, na.rm=TRUE)),
        'Time since previous review' = min(time_since_review)
      ), by = "place_id"]
)
data_all[, 'Relative review likes count' := rank(review_median_of_likes, na.last="keep") / sum(!is.na(review_median_of_likes)), by="keyword_strings"]

#features from q_a
data_all <- merge(data_all, by="place_id", all.x=T, all.y=F,
                  q_a[, list(
                    'Question answer ratio' = sum(question_answer == "") / .N
                  ), by = "place_id"]
)

#features from cities
data_all[, city_population := 0.0]
data_all[, city_density := 0.0]
#find closest matching location in cities data given by Euclidean distance between lat&long
for(i in 1:nrow(data_all)) {
  pop <- NA_real_
  den <- NA_real_
  if(!is.na(data_all$latitude[i])) {
    city_index <- which.min((data_all$latitude[i] - cities$lat)^2 + (data_all$longtitude[i] - cities$lng)^2)
    pop <- cities$population[city_index]
    den <- cities$density[city_index]
  }
  data_all[i, city_population := pop]
  data_all[i, city_density := den]
 }
data_all[, 'Relative place population' := rank(city_population, na.last="keep") / sum(!is.na(city_population)), by="keyword_strings"]
data_all[, 'Relative place population density' := rank(city_density, na.last="keep") / sum(!is.na(city_density)), by="keyword_strings"]


#features from most_relevant_reviews
data_all <- merge(data_all, by="place_id", all.x=T, all.y=F,
                  most_relevant_reviews[, list(
                    'Mean of most relevant ratings' = mean(most_relevant_reviews_rating == 5)
                  ), by = "place_id"]
)

#features from backlink_data_websites
data_all <- merge(data_all, backlink_data_websites, by="place_id", all.x=T, all.y=F)
data_all[, 'Relative domain_rating' := rank(domain_rating, na.last="keep") / sum(!is.na(domain_rating)), by=keyword_strings]
data_all[, 'Relative ahrefs_rank' := rank(ahrefs_rank, na.last="keep") / sum(!is.na(ahrefs_rank)), by=keyword_strings]
data_all[, 'Relative ref_domains_total' := rank(ref_domains_total, na.last="keep") / sum(!is.na(ref_domains_total)), by=keyword_strings]
data_all[, 'Relative ref_domains_dofollow' := rank(ref_domains_dofollow, na.last="keep") / sum(!is.na(ref_domains_dofollow)), by=keyword_strings]
data_all[, 'Relative linked_domains' := rank(linked_domains, na.last="keep") / sum(!is.na(linked_domains)), by=keyword_strings]
data_all[, 'Relative total_backlinks' := rank(total_backlinks, na.last="keep") / sum(!is.na(total_backlinks)), by=keyword_strings]
data_all[, 'Relative backlinks_no_follow' := rank(backlinks_no_follow, na.last="keep") / sum(!is.na(backlinks_no_follow)), by=keyword_strings]
data_all[, 'Relative total_keywords' := rank(total_keywords, na.last="keep") / sum(!is.na(total_keywords)), by=keyword_strings]
data_all[, 'Relative total_traffic' := rank(total_traffic, na.last="keep") / sum(!is.na(total_traffic)), by=keyword_strings]


#for xgboost rank modeling, rows have to be ordered by group
setorder(data_all, keyword_strings, position)

```


## Modelling

The goal of the study is to find answers to three key questions:
1. How accurately can the rankings be predicted given the dependent variables?
2. What are the most important features for the predictions?
3. What is the direction of the impact?

The method of choice for the study was gradient boosted decision trees (GBDT). GBDT is a widely used machine learning technique which can be used in many applications from regression or classification to learning to rank type of problems. In a learning to rank problem, there is a ordered list of items and the goal for the model is to calculate a score for each item based on dependent variables such that the original order is retained.

In building the model, dataset was split to two folds: train data (containing ~70% of searches) and test data (the rest of the data, about 30%). GBDT model was fitted using training data, predictions were calculated for the test data set, and then finally, predictions were compared to real observed rankings. The chosen evaluation metric was Spearman's rank correlation coefficient. Spearman's rank correlation is a scaled measurement of the agreement of two rankings. Perfectly matching rankings would give value of 1, the expected value for random rankings is zero and reverse order would have value of -1.

The next step is to understand why the model makes particular predictions; what are the most important dependent variables and how their values effect the predictions? For this purpose SHapley Additive exPlanations (SHAP) values were calculated. SHAP each prediction is presented a sum of each dependent variable's responsibility. The overall impact can be measured as average of absolute values over the whole dataset.


```{r model, cache=TRUE, echo = FALSE}
#split data to train and test sets based on search term
set.seed(1)
train_ratio <- 0.7
ids <- unique(data_all$keyword_strings)
train_ids <- sample(ids, floor(train_ratio*length(ids)))
test_ids <- setdiff(ids, train_ids)

feature_cols <- c(
'Relative review rating',
'Relative reviews count',
'Relative photos count',
'Relative questions count',

'Google Ads',
'Provides updates',
'Is business claimed',

'Has street address',
'Has website',
'Has phone',
'Has twitter',
'Has fb',
'Has youtube',
'Has instagram',
'Has linkedin',

'Keywords in description count',
'Keywords in title count',
'Has city in description',
'Has city in title' ,
'Has lawyer or attorney in title',
'Has lawyer or attorney in description',
'Has car in description',
'Has car in title' ,
'Has injury in description',
'Has injury in title',
'Character count in description', 
'Character count in title', 

'Type category personal injury attorney', 
'Type category lawyer', 
'Type category law firm',
'Type category empty',
'Type category criminal justice attorney',
'Type category other', 

'Has same city listed as in search query',

'Has featured reviews', 

'Ratio of 5s in ratings', 
'Ratio of 4s in ratings',
'Ratio of 3s in ratings',
'Ratio of 2s in ratings',
'Ratio of 1s in ratings',
'Review response ratio',
'Time since previous review',

'Relative review likes count',

'Question answer ratio',

'Relative domain_rating',
'Relative ahrefs_rank',
'Relative ref_domains_total',
'Relative ref_domains_dofollow',
'Relative linked_domains',
'Relative total_backlinks',
'Relative backlinks_no_follow',
'Relative total_keywords',
'Relative total_traffic',

'Relative place population',
'Relative place population density',

'Mean of most relevant ratings')

data_train <- data_all[keyword_strings %in% train_ids, ]
X_train <- as.matrix(data_train[, feature_cols, with=F])
y_train <- data_all[keyword_strings %in% train_ids]$target
group_train <- unique(data_all[keyword_strings %in% train_ids,c("keyword_strings", "group_size")])$group_size

data_test <- data_all[keyword_strings %in% test_ids, ]
X_test <- as.matrix(data_test[, feature_cols, with=F])
y_test <- data_all[keyword_strings %in% test_ids]$target
group_test <- unique(data_all[keyword_strings %in% test_ids,c("keyword_strings", "group_size")])$group_size


#model settings
dtrain = xgb.DMatrix(data = X_train, label = y_train, group = group_train)
dtest = xgb.DMatrix(data = X_test, label = y_test, group = group_test)

params <- list(eta = 0.1, 
               subsample = 0.7, 
               colsample_bytree = 0.7, 
               min_child_weight = 1,
               max_leaves = 64,
               max_depth = 0,
               lambda = 0,
               alpha = 0.5,
               grow_policy = "lossguide",
               tree_method = "hist",
               objective = "rank:pairwise"
          )
nrounds <- 1000

#model training
model <- xgb.train(data = dtrain, 
                 params = params, 
                 nrounds = nrounds, 
                 verbose = 0, 
                 watchlist = list(train=dtrain, test=dtest))

#predict
pred <- predict(model, X_test)

#shap
shap_values <- shap.values(xgb_model = model, X_train = X_train)
shap_importance <- data.frame(feature = names(shap_values$mean_shap_score), 
                              importance = shap_values$mean_shap_score,
                              stringsAsFactors = F)

shap_values_test <- shap.values(model, X_test)
shap_scores_test <- shap_values_test$shap_score
```

## Results

#### Accuracy
* Spearman's rank correlation
  + Scaled measurement (values in [-1,1]) of the agreement of two rankings (here, observed vs predicted)
  + Perfect prediction of rankings would get value 1
  + Expected value of randomly assigned ranking is 0
  + Reverse order ranking is -1
  + In the plot below, showing the distribution of correlations calculated separately for each search  
  

```{r accuracy, cache=TRUE, echo = FALSE}
dc <- data_all[keyword_strings %in% test_ids,]
dc[, pred := pred]
dc <- dc[, list(correlation=cor(target, pred, method="spearman")), by="keyword_strings"]
mean_correlation <- format(round(mean(dc$correlation), 2), nsmall = 2)
prediction_plot <- ggplot(dc, aes(x = correlation)) + 
  geom_histogram(fill="darkblue", color="white", binwidth=0.05) +
  xlab("Correlations for search rankings") +
  ylab("Count") + 
  labs(title = paste0("Mean Spearman's correlations between predicted and observed rankings: ", mean_correlation))
plot(prediction_plot)
```

### Feature importance
* SHapley Additive exPlanations (SHAP):
  + Tries to explain model's predictions as a sum of each feature's responsibility 
  + First plot shows average value of each features contribution over the whole dataset and can be interpreted as a overall importance for predictions
  + Second plot shows the direction of the impact given feature's value. E.g. if the attribute google_ad got high value (high="Yes, low="No"), it's SHAP value is also highly positive; meaning it will increase the predicted ranking score, where as low value doesn't change much of anything. 
  + https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d
  + https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30
  + https://arxiv.org/abs/1705.07874

```{r importance, cache=TRUE, echo = FALSE}

importance_plot <- ggplot(shap_importance, aes(x=reorder(feature, importance), y=importance, fill=importance)) +
  geom_bar(stat='identity') + 
  scale_fill_gradient2(low = "red", mid ="yellow", high = "green", midpoint = 0) + 
  xlab("Feature") +
  ylab("Importance") + 
  labs(title = "SHAP feature importance") + 
  theme(plot.title = element_text(hjust = 0.5), legend.position="none", axis.text.x=element_blank(), axis.ticks.x=element_blank()) + scale_y_continuous(limits=c(0.0,NA))+
  coord_flip()
plot(importance_plot)

shap.plot.summary.wrap2(shap_values$shap_score, X_train, dilute=10)
```

### Case studies
```{r case, echo=FALSE}
dc <- dc[order(dc$correlation),]

create_result_dataframe <- function(key) {
  rows <- which(data_test$keyword_strings == key)
  res <- data.frame(feature1 = rep("", length(rows)),
                    value1 = rep(0, length(rows)),
                    shap1 = rep(0, length(rows)),
                    feature2 = rep("", length(rows)),
                    value2 = rep(0, length(rows)),
                    shap2 = rep(0, length(rows)),
                    predicted_score = rep(0, length(rows)),
                    predicted_rank = rep(0, length(rows)),
                    real_rank = rep(0, length(rows)),stringsAsFactors = F)
  features <- names(shap_scores_test)
  for(i in 1:length(rows)) {
    values <- unlist(shap_scores_test[rows[i]])
    top2 <- order(abs(values), decreasing=TRUE)[1:2]
    res[i, 1] <- features[top2[1]]
    res[i, 2] <- data_test[rows[i], features[top2[1]], with=F]
    res[i, 3] <- values[top2[1]]
    
    res[i, 4] <- features[top2[2]]
    res[i, 5] <- data_test[rows[i], features[top2[2]], with=F]
    res[i, 6] <- values[top2[2]]
    
    res[i, 7] <- sum(values)
    res[i, 9] <- y_best[i]
  }
  res[,8] <- rank(-res$predicted_score)
  return(res)
}

best_case <- dc$keyword_strings[nrow(dc)]
best_correlation <- format(round(dc$correlation[nrow(dc)], 2), nsmall = 2)
X_best <- X_test[data_test$keyword_string == best_case,]
y_best <- data_test[keyword_strings == best_case]$position
scores_best <- predict(model, X_best)
df_best <- create_result_dataframe(best_case)

median_case <- dc$keyword_strings[floor(nrow(dc)/2)]
median_correlation <- format(round(dc$correlation[floor(nrow(dc)/2)], 2), nsmall = 2)
X_median <- X_test[data_test$keyword_string == median_case,]
y_median <- data_test[keyword_strings == median_case]$position
scores_median <- predict(model, X_median)
df_median <- create_result_dataframe(median_case)

worst_case <- dc$keyword_strings[1]
worst_correlation <- format(round(dc$correlation[1], 2), nsmall = 2)
X_worst <- X_test[data_test$keyword_string == worst_case,]
y_worst <- data_test[keyword_strings == worst_case]$position
scores_worst <- predict(model, X_worst)
df_worst <- create_result_dataframe(worst_case)
```

#### The most accurate predictions
```{r best, results='asis', echo=FALSE}
plot(y_best, scores_best, xlab="Observed rank", ylab="Predicted score", main=best_case, sub = paste0("Spearman's correlation: ", best_correlation))
knitr::kable(df_best, caption = "Most important features and their impact on predictions for each seach result, ordered by position.")
```

#### Average case
```{r median, results='asis', echo=FALSE}
plot(y_median, scores_median, xlab="Observed rank", ylab="Predicted score", main=median_case, sub = paste0("Spearman's correlation: ", median_correlation))
knitr::kable(df_median, caption = "Most important features and their impact on predictions for each seach result, ordered by position.")
```

#### The worst predictions
```{r worst, results='asis', echo=FALSE}
plot(y_worst, scores_worst, xlab="Observed rank", ylab="Predicted score", main=worst_case, sub = paste0("Spearman's correlation: ", worst_correlation))
knitr::kable(df_worst, caption = "Most important features and their impact on predictions for each seach result, ordered by position.")
```