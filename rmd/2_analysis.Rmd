---
title: "2_analysis"
output: html_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r results=FALSE, message=FALSE, echo=FALSE}
  save <- T
  #create plots folder, if it doesn't exist
  if(save && !dir.exists(here::here("plots"))) {
      dir.create(here::here("plots"))
  }

  #load required packages, if package is not found try to install
  required_packages <- c("xgboost", "data.table", "stringr", "SHAPforxgboost", "ggplot2", "gridExtra", "wordcloud", "tm", "grDevices", "here", "pdftools")

  not_installed_packages <- required_packages[!(required_packages %in% installed.packages()[,"Package"])]
  if(length(not_installed_packages) > 0) 
    install.packages(not_installed_packages)
  was_succesfully_loaded <- sapply(required_packages, require, character.only = TRUE)
  
  if(any(!was_succesfully_loaded)) {
    cat(required_packages[!was_succesfully_loaded],"\n")
  }
```

## To-Do-List


## Data

Google My Business search results were collected for 4 unique keyword combinations in 426 US cities, giving in total 1659 search results. The format of the search queries was one the following:

* (city) + " car accident lawyer"  
* (city) + " personal injury lawyer" 
* (city) + " car accident attorney"
* (city) + " personal injury attorney"

#### Independent variables
All variables with prefix "Relative" are calculated as rank(values) / length(values) for values inside each search group. So for example, search with keyword "Milwaukee car accident lawyer" the entry with highest number of photos would get Relative number of photos value equal to 1, entry with lowest value would get 0, and the rest of the values would be something in between. The motivation behind this transformation is to make attribute values more comparable between searches, i.e. trying to minimize effect of the size of the population of the city etc.

* Attributes from basic.csv
  + Review rating
  + Relative #reviews
  + Relative #photos
  + Relative #questions
  + Provides updates
  + Is business claimed
  + Has street address
  + Has website
  + Has phone
  + #Keywords in description
  + #Keywords in title
  + Has city in description
  + Has city in title 
  + Has lawyer or attorney in title
  + Has lawyer or attorney in description
  + Has car in description
  + Has car in title 
  + Has injury in description
  + Has injury in title
  + #Characters in description 
  + #Characters in title 
  + Type category is personal injury attorney 
  + Type category is lawyer 
  + Type category is law firm
  + Type category is empty
  + Type category is criminal justice attorney
  + Type category is other 

* Attributes from social_profile.csv
  + Has twitter
  + Has fb
  + Has youtube
  + Has instagram
  + Has linkedin
   
* Attributes from all_reviews.csv
  + Ratio of 5s in ratings 
  + Ratio of 4s in ratings
  + Ratio of 3s in ratings
  + Ratio of 2s in ratings
  + Ratio of 1s in ratings
  + Review response ratio
  + Relative #review likes
  
* Attributes from q_a.csv
  + Question answer ratio
  
* Attributes from backlink_data_websites.csv
  + Relative domain_rating
  + Relative ahrefs_rank
  + Relative ref_domains_total
  + Relative ref_domains_dofollow
  + Relative linked_domains
  + Relative total_backlinks
  + Relative backlinks_no_follow
  + Relative total_keywords
  + Relative total_traffic

#### Dependent variables
* Position in the search results

```{r data, cache=TRUE, echo = FALSE}


#read csv
here::here("raw_data", "GMB data", "basic.csv")
basic <- fread(here::here("raw_data", "GMB data", "basic.csv"))
social_profile <- fread(here::here("raw_data", "GMB data", "social_profile.csv"))
user_review_featured <- fread(here::here("raw_data", "GMB data", "user_review_featured.csv"))
q_a <- fread(here::here("raw_data", "GMB data", "q_a.csv"))
all_reviews <- fread(here::here("raw_data", "GMB data", "all_reviews.csv"))
backlink_data_websites <- fread(here::here("raw_data", "GMB data", "backlink_data_websites.csv"))
cities <- fread(here::here("raw_data", "uscities_raw.csv"))
most_relevant_reviews <- fread(here::here("raw_data", "GMB data", "most_relevant_reviews.csv"))

#keep only the latest scraped rows for social_profile
social_profile[, scraping_date := as.Date(scraping_date)]
social_profile <- social_profile[, .SD[which.max(scraping_date)], by="place_id"]

#function to return relative values,
#ie. return new values between 0 and 1; zero for lowest and one for highest original value and the rest something between
relative_value <- function(values, na_as_zero = FALSE) {
  res <- (rank(values, na.last="keep", ties.method="average") - 1) / (sum(!is.na(values))-1)
  if(na_as_zero) {
    res[is.na(values)] <- 0
  }
  return(res)
}


#start processing data for modeling, create/transform attributes etc
data_all <- merge(basic, social_profile[, c("place_id","twitter","fb","youtube","instagram","linkedin"),with=F], by="place_id", all.x=T)
data_all[, target := relative_value(-position), by=keyword_strings]
data_all[, relative_position := relative_value(position), by=keyword_strings]

#remove result with google_ad =="Yes". These row are always in position 1 and 2 of the results
data_all <- data_all[google_ad == "No"]

#calculate search group sizes and remove a few searches without enough result
data_all[, group_size := .N, by=keyword_strings]
data_all <- data_all[group_size >= 10]

#relative variables, ie. normalize by dividing by median value of the whole search group
data_all[, 'Review rating' := ifelse(is.na(rating) | rating < 1 | rating > 5, NA_real_, rating), by=keyword_strings]
data_all[, 'Relative #reviews' := relative_value(reviews, T), by=keyword_strings]
data_all[, 'Relative #photos' := relative_value(number_of_photos, T), by=keyword_strings]
data_all[, 'Relative #questions' := relative_value(total_questions, T), by=keyword_strings]

#transform Yes/No to 1/0
#data_all[, 'Has GoogleAd' := as.integer(google_ad == "Yes")]
data_all[, 'Provides updates' := as.integer(provides_updates == "Yes")]
data_all[, 'Is business claimed' := as.integer(business_claimed == "Yes")]

#does attribute have content or is it empty)
data_all[, 'Has street address' := as.integer(street_address != "")]
data_all[, 'Has website' := as.integer(website != "")]
data_all[, 'Has phone' := as.integer(phone != "")]
data_all[, 'Has twitter' := as.integer(twitter != "")]
data_all[, 'Has fb' := as.integer(fb != "")]
data_all[, 'Has youtube' := as.integer(youtube != "")]
data_all[, 'Has instagram' := as.integer(instagram != "")]
data_all[, 'Has linkedin' := as.integer(linkedin != "")]

#description & title
data_all[, '#Keywords in description' := sum(str_count(description, unlist(str_split(keyword_strings, " ")))) , by = row.names(data_all)]
data_all[, '#Keywords in title' := sum(str_count(title, unlist(str_split(keyword_strings, " ")))) , by = row.names(data_all)]
data_all[, 'Has city in description' := as.integer(grepl(city, description, ignore.case = TRUE)), by = row.names(data_all)]
data_all[, 'Has city in title' := as.integer(grepl(city, title, ignore.case = TRUE)), by = row.names(data_all)]
data_all[, 'Has lawyer or attorney in title' := as.integer(grepl("lawyer|attorney", title, ignore.case = TRUE)), by = row.names(data_all)]
data_all[, 'Has lawyer or attorney in description' := as.integer(grepl("lawyer|attorney", description, ignore.case = TRUE)), by = row.names(data_all)]
data_all[, 'Has car accident or personal injury in description' := as.integer(grepl("car accident|personal injury", description, ignore.case = TRUE)), by = row.names(data_all)]
data_all[, 'Has car accident or personal injury in title' := as.integer(grepl("car accident|personal injury", title, ignore.case = TRUE)), by = row.names(data_all)]
data_all[, '#Characters in description' := nchar(description)]
data_all[, '#Characters in title' := nchar(title)]

#type_category features
common_types <- c("Personal injury attorney",
                  "Lawyer",
                  "Law firm",
                  "",
                  "Criminal justice attorney" 
)
data_all[, 'Type category is personal injury attorney' := as.numeric(type_category == common_types[1])]
data_all[, 'Type category is lawyer' := as.numeric(type_category == common_types[2])]
data_all[, 'Type category is law firm' := as.numeric(type_category == common_types[3])]
data_all[, 'Type category is empty' := as.numeric(type_category == common_types[4])]
data_all[, 'Type category is criminal justice attorney' := as.numeric(type_category == common_types[5])]
data_all[, 'Type category is other' := as.numeric(!(type_category %in% common_types))]

data_all[, 'Has same city listed as in search query' := as.integer(city == city_place)]

#features from user_review_featured
data_all[, 'Has featured reviews' := as.integer(place_id %in% unique(user_review_featured$place_id))]

#features from all_reviews
all_reviews[, time_since_review := 0]
all_reviews[grepl("minute", all_reviews_date), time_since_review := 1]
all_reviews[grepl("hour", all_reviews_date), time_since_review := 2]
all_reviews[grepl("day", all_reviews_date), time_since_review := 3]
all_reviews[grepl("week", all_reviews_date), time_since_review := 4]
all_reviews[grepl("month", all_reviews_date), time_since_review := 5]
all_reviews[grepl("year", all_reviews_date), time_since_review := 6]

data_all <- merge(data_all, by="place_id", all.x=T, all.y=F,
      all_reviews[, list(
        'Ratio of 5s in ratings' = sum(all_reviews_rating == 5) / .N,
        'Ratio of 4s in ratings' = sum(all_reviews_rating == 4) / .N,
        'Ratio of 3s in ratings' = sum(all_reviews_rating == 3) / .N,
        'Ratio of 2s in ratings' = sum(all_reviews_rating == 2) / .N,
        'Ratio of 1s in ratings' = sum(all_reviews_rating == 1) / .N,
        'Review response ratio' = sum(all_reviews_response_from_owner != "") / .N,
        'review_median_of_likes' = as.numeric(median(all_reviews_number_of_likes, na.rm=TRUE)),
        'Time since previous review' = min(time_since_review)
      ), by = "place_id"]
)
data_all[, 'Relative #review likes' := relative_value(review_median_of_likes, T), by="keyword_strings"]

#features from q_a
data_all <- merge(data_all, by="place_id", all.x=T, all.y=F,
                  q_a[, list(
                    'Question answer ratio' = sum(question_answer == "") / .N
                  ), by = "place_id"]
)

#features from cities
data_all[, city_population := 0.0]
data_all[, city_density := 0.0]
#find closest matching location in cities data given by Euclidean distance between lat&long
for(i in 1:nrow(data_all)) {
  pop <- NA_real_
  den <- NA_real_
  if(!is.na(data_all$latitude[i])) {
    city_index <- which.min((data_all$latitude[i] - cities$lat)^2 + (data_all$longtitude[i] - cities$lng)^2)
    pop <- cities$population[city_index]
    den <- cities$density[city_index]
  }
  data_all[i, city_population := pop]
  data_all[i, city_density := den]
 }
data_all[, 'Relative place population' := relative_value(city_population), by="keyword_strings"]


#features from most_relevant_reviews
data_all <- merge(data_all, by="place_id", all.x=T, all.y=F,
                  most_relevant_reviews[, list(
                    'Mean of most relevant ratings' = mean(most_relevant_reviews_rating == 5)
                  ), by = "place_id"]
)

#features from backlink_data_websites
data_all <- merge(data_all, backlink_data_websites, by="place_id", all.x=T, all.y=F)
data_all[, 'Relative domain_rating' := relative_value(domain_rating), by=keyword_strings]
data_all[, 'Relative ahrefs_rank' := relative_value(ahrefs_rank), by=keyword_strings]
data_all[, 'Relative ref_domains_total' := relative_value(ref_domains_total), by=keyword_strings]
data_all[, 'Relative ref_domains_dofollow' := relative_value(ref_domains_dofollow), by=keyword_strings]
data_all[, 'Relative linked_domains' := relative_value(linked_domains), by=keyword_strings]
data_all[, 'Relative total_backlinks' := relative_value(total_backlinks), by=keyword_strings]
data_all[, 'Relative backlinks_no_follow' := relative_value(backlinks_no_follow), by=keyword_strings]
data_all[, 'Relative total_keywords' := relative_value(total_keywords), by=keyword_strings]
data_all[, 'Relative total_traffic' := relative_value(total_traffic), by=keyword_strings]

```


## Modelling

The goal of the statistical modelling in this study is to find answers to three key questions:

1. How accurately can the rankings be predicted given the dependent variables?
2. What are the most important features for the predictions?
3. What is the direction of the impact?

The method of choice for the study was gradient boosted decision trees (GBDT). GBDT is a widely used machine learning technique which can be used in many settings from regression or classification to learning to rank type of problems. In a learning to rank problem, there is a ordered list of items and the goal for the model is to calculate a score for each item based on dependent variables such that the original order is retained. 

In process of building the model, data set was split to two folds: train data (containing around 70% of searches) and test data (the rest of the data, about 30%). GBDT model was fitted using training data, predictions were calculated for the test data set, and then finally predictions were compared to real observed rankings. The chosen evaluation metric was Spearman's rank correlation coefficient. Spearman's rank correlation is a scaled measurement of the agreement of two rankings. Perfectly matching rankings would give value of 1, the expected value for random rankings is zero and reverse order would have value of -1.

The next step is to understand why the model makes particular predictions; what are the most important dependent variables and how their values effect the predictions? For this purpose SHapley Additive exPlanations (SHAP) values were calculated. In SHAP each prediction is presented as a sum of each dependent variable's responsibility. Then the overall impact of any particular variable can be measured as a average of absolute values over the whole data set.


```{r model, cache=TRUE, echo = FALSE}
#for xgboost rank modeling, rows have to be ordered by group
setorder(data_all, keyword_strings, position)

#split data to train and test sets based on search term
set.seed(1)
train_ratio <- 0.7
ids <- unique(data_all$keyword_strings)
train_ids <- sample(ids, floor(train_ratio*length(ids)))
test_ids <- setdiff(ids, train_ids)

feature_cols <- c(
'Review rating',
'Relative #reviews',
'Relative #photos',
'Relative #questions',

#'Has GoogleAd',
'Provides updates',
'Is business claimed',

'Has street address',
'Has website',
'Has phone',
'Has twitter',
'Has fb',
'Has youtube',
'Has instagram',
'Has linkedin',

'#Keywords in description',
'#Keywords in title',
'Has city in description',
'Has city in title' ,
'Has lawyer or attorney in title',
'Has lawyer or attorney in description',
'Has car accident or personal injury in description',
'Has car accident or personal injury in title' ,
'#Characters in description', 
'#Characters in title', 

'Type category is personal injury attorney', 
'Type category is lawyer', 
'Type category is law firm',
'Type category is empty',
'Type category is criminal justice attorney',
'Type category is other', 

'Has same city listed as in search query',

'Has featured reviews', 

'Ratio of 5s in ratings', 
'Ratio of 4s in ratings',
'Ratio of 3s in ratings',
'Ratio of 2s in ratings',
'Ratio of 1s in ratings',
'Review response ratio',
'Time since previous review',

'Relative #review likes',

'Question answer ratio',

'Relative domain_rating',
'Relative ahrefs_rank',
'Relative ref_domains_total',
'Relative ref_domains_dofollow',
'Relative linked_domains',
'Relative total_backlinks',
'Relative backlinks_no_follow',
'Relative total_keywords',
'Relative total_traffic',

'Relative place population',
'Mean of most relevant ratings')

data_train <- data_all[keyword_strings %in% train_ids, ]
X_train <- as.matrix(data_train[, feature_cols, with=F])
y_train <- data_all[keyword_strings %in% train_ids]$target
group_train <- unique(data_all[keyword_strings %in% train_ids,c("keyword_strings", "group_size")])$group_size

data_test <- data_all[keyword_strings %in% test_ids, ]
X_test <- as.matrix(data_test[, feature_cols, with=F])
y_test <- data_all[keyword_strings %in% test_ids]$target
group_test <- unique(data_all[keyword_strings %in% test_ids,c("keyword_strings", "group_size")])$group_size


#model settings
dtrain = xgb.DMatrix(data = X_train, label = y_train, group = group_train)
dtest = xgb.DMatrix(data = X_test, label = y_test, group = group_test)

params <- list(eta = 0.1, 
               subsample = 0.7, 
               colsample_bytree = 0.7, 
               min_child_weight = 1,
               max_leaves = 64,
               max_depth = 0,
               lambda = 0,
               alpha = 0.5,
               grow_policy = "lossguide",
               tree_method = "hist",
               objective = "rank:pairwise"
          )
nrounds <- 1000

#model training
model <- xgb.train(data = dtrain, 
                 params = params, 
                 nrounds = nrounds, 
                 verbose = 0, 
                 watchlist = list(train=dtrain, test=dtest))

#predict
pred <- predict(model, X_test)

#shap
shap_values <- shap.values(xgb_model = model, X_train = X_train)
shap_importance <- data.frame(feature = names(shap_values$mean_shap_score), 
                              importance = shap_values$mean_shap_score,
                              stringsAsFactors = F)

shap_values_test <- shap.values(model, X_test)
shap_scores_test <- shap_values_test$shap_score
```

## Results

#### Accuracy

* In the plot below, showing the distribution of correlations calculated separately for each search
* Overall mean correlation is about 0.6, showing fairly good fit between observations and predictions

```{r accuracy, echo = FALSE}
dc <- data_all[keyword_strings %in% test_ids,]
dc[, pred := pred]
dc <- dc[, list(correlation=cor(target, pred, method="spearman")), by="keyword_strings"]
mean_correlation <- format(round(mean(dc$correlation), 2), nsmall = 2)
spearman_plot <- ggplot(dc, aes(x = correlation)) + 
  geom_histogram(fill="steelblue", color="white", binwidth=0.05) +
  xlab("Correlations for search rankings") +
  ylab("Count") + 
  labs(title = paste0("Mean Spearman's correlations between predicted and observed rankings: ", mean_correlation))
plot(spearman_plot)

if(save == T){ 
  ggsave(here::here("plots", "spearman.pdf"), spearman_plot, 
         width = 12.5, height = 8, device = cairo_pdf) 
}

```

### Feature importance

* The first plot is showing feature importance, that is, each feature's average contribution to model's predictions. 
  
* Second plot shows the direction of the impact given feature's value. 

* Overall, the most interesting features for us to look at are a) high in the first plot and b) show clear pattern in the second plot. 

* E.g. if we look at the first row and the feature named "Has same city listed as in search query", we can see kind of polarized distribution of SHAP values around zero. Yellow points correspond to high feature values (in this case, "No") and here their impact to all predictions in the data set is negative and therefore making model belief that predicted positions should be worse for them. Where as purple points correspond to high feature values ("Yes") and have positive impact for predicted positions.

* Similarly, "Type category is personal injury" and "Type category is personal injury" are similar too the "Has same city listed as in search query". Ie. if they have value "Yes" they will impact positively to predicted positions.

```{r importance, echo = FALSE}

feature_importance_plot <- ggplot(shap_importance, aes(x=reorder(feature, importance), y=importance, fill=importance)) +
  geom_bar(stat='identity') + 
  scale_fill_gradient2(low = "red", mid ="yellow", high = "green", midpoint = 0) + 
  xlab("Feature") +
  ylab("Importance") + 
  labs(title = "SHAP feature importance") + 
  theme(plot.title = element_text(hjust = 0.5), legend.position="none", axis.text.x=element_blank(), axis.ticks.x=element_blank()) + scale_y_continuous(limits=c(0.0,NA))+
  coord_flip()
plot(feature_importance_plot)

shap_values_plot <- shap.plot.summary.wrap2(shap_values$shap_score, X_train, dilute=3)
plot(shap_values_plot)

if(save == T){ 
  ggsave(here::here("plots", "feature_importance.pdf"), feature_importance_plot, 
         width = 12.5, height = 8, device = cairo_pdf) 
  ggsave(here::here("plots", "shap_values.pdf"), shap_values_plot, 
         width = 12.5, height = 8, device = cairo_pdf) 
}

```


## A closer look at the features

The depended variables used in this study can be roughly organized into five main groups, these are listed below and also showing a few important variables suggested by SHAP values.

* Location
  + Has same city listed as in search query
  + Relative place population

* Popularity
  + Relative #photos 
  + Relative #questions

* Type category
  + Type category is personal injury attorney/lawyer

* Keywords and title/description
  + Has "lawyer or attorney"/city in title
  + #Characters in description

* Reviews
  + Relative #reviews
  + Review rating

In terms of SEO, the first two categories are not much of a interest as they are something difficult or even impossible to change or adjust, but the last three are more interesting and worth further investigation. 

```{r features, results='asis', echo=FALSE}
#data for all visualizations
#to make it cleaner to make plots, keep only searches with exactly 20 results. 
#Ie. keep only searches with >= 20 results and then if more than 20, remove the extra rows
#keeps about 90% of all rows
data_vis <- data_all[group_size >= 20,]
data_vis <- data_vis[, .SD[order(position) <= 20], by="keyword_strings"]
data_vis <- data_vis[, position := rank(position, ties.method="first"), by="keyword_strings"]

#helper function for printing
to_percentage <- function(x, digits=2) {paste0(round(100*x, digits=digits), "%")}

```  
### Type category
```{r type_category_table, results='asis', echo=FALSE}

#gather basic info
type_counts <- as.data.frame(sort(table(data_vis[type_category!=""]$type_category), decreasing=T), stringsAsFactors = F)
names(type_counts)[1] <- "category"
type_unique_data <- data_vis[, list(N = length(unique(type_category))), by="keyword_strings"]

#build table
rs <- list()
rs[[length(rs)+1]] <- list('Total unique categories', 
                           length(unique(data_all$type_category)) - 1)

rs[[length(rs)+1]] <- list('Missing type category', 
                         to_percentage(sum(data_all$type_category=="") / nrow(data_all)))

rs[[length(rs)+1]] <- list('Categories with more than >=10 results', 
                         sum(type_counts$Freq>=10))

rs[[length(rs)+1]] <- list('Categories with more than >=100 results', 
                           sum(type_counts$Freq>=100))

rs[[length(rs)+1]] <- list('Categories with more than >=1000 results', 
                           sum(type_counts$Freq>=1000))

rs[[length(rs)+1]] <- list('Median unique categories in one search', 
                           median(type_unique_data$N))

rs[[length(rs)+1]] <- list('Min unique categories in one search', 
                           min(type_unique_data$N))

rs[[length(rs)+1]] <- list('Max unique categories in one search', 
                           max(type_unique_data$N))

type_table <- rbindlist(rs)
setnames(type_table, 1:2, c("Type", "Value"))

knitr::kable(type_table, caption = "Basic information about type categories")
``` 

```{r type_category_plots, results='asis', echo=FALSE}
#plots
keep_top_N <- 10
type_freq_plot <- ggplot(data=type_counts[1:keep_top_N,], aes(x=reorder(category, Freq), y=Freq)) +
  geom_bar(stat="identity", fill="steelblue") +
  ylab("Count") +
  xlab("Category") + 
  labs(title = "Ten most common type categories in search results") +
  theme(plot.title = element_text(hjust = 0.5)) +
  coord_flip()
print(type_freq_plot)

ps <- list()
for(i in 1:6) {
  ps[[i]] <- ggplot(data_vis[type_category == type_counts$category[i]], aes(x=position)) +
  geom_histogram(color="white", fill="steelblue", bins=20) +
  ylab("Count") +
  xlab("Position") + 
  labs(title = type_counts$category[i]) + 
  theme(plot.title = element_text(hjust = 0.5), axis.text.x=element_blank(), axis.ticks.x=element_blank())
}
type_position_plot <- grid.arrange(ps[[1]], ps[[2]], ps[[3]],
             ps[[4]], ps[[5]], ps[[6]], nrow=2, ncol=3, top = "Distribution of search positions for 6 most common type categories")

if(save == T){ 
  ggsave(here::here("plots", "type_freq.pdf"), type_freq_plot, 
         width = 12.5, height = 8, device = cairo_pdf) 
  ggsave(here::here("plots", "type_position.pdf"), type_position_plot, 
         width = 12.5, height = 8, device = cairo_pdf) 
}
```    

Key takeaways:

* Distributions for more general categories (Lawyer, Law firm, Legal services) are tilted towards lower positions in the results compared to best matching category (Personal injury attorney).
* Same thing occurs with specific, but less matching categories (Criminal justice attorney, Family law attorney)
* However, some of these categories (Law firm, Criminal justice attorney, Legal services) have relatively large counts for top positions. One possible explanation is that these groups represent larger companies with high revenues and therefore would be ranked higher. 

### Titles and descriptions
```{r title_description_table, results='asis', echo=FALSE, message=FALSE}


#gather basic info about titles & descriptions

rs <- list()
rs[[length(rs)+1]] <- list("Median character length (non missing)", 
                           median(data_all[title!=""]$`#Characters in title`),
                           median(data_all[description!=""]$`#Characters in description`))

rs[[length(rs)+1]] <- list("Min character length (non missing)", 
                           min(data_all[title!=""]$`#Characters in title`),
                           min(data_all[description!=""]$`#Characters in description`))

rs[[length(rs)+1]] <- list("Max character length (non missing)",
                           max(data_all[title!=""]$`#Characters in title`),
                           max(data_all[description!=""]$`#Characters in description`))

rs[[length(rs)+1]] <- list('Missing', 
                          to_percentage(sum(data_all$title=="") / nrow(data_all)),
                          to_percentage(sum(data_all$description=="") / nrow(data_all)))

rs[[length(rs)+1]] <- list("Containing lawyer or attorney", 
                           to_percentage(sum(data_all$`Has lawyer or attorney in title`==1) / nrow(data_all)),
                          to_percentage(sum(data_all$`Has lawyer or attorney in description`==1) / nrow(data_all)))

rs[[length(rs)+1]] <- list("Containing car accident or personal injury", 
                          to_percentage(sum(data_all$`Has car accident or personal injury in title`==1) / nrow(data_all)),
                          to_percentage(sum(data_all$`Has car accident or personal injury in description`==1) / nrow(data_all)))

rs[[length(rs)+1]] <- list("Containing city name", 
                           to_percentage(sum(data_all$`Has city in title`==1) / nrow(data_all)),
                           to_percentage(sum(data_all$`Has city in description`==1) / nrow(data_all)))

title_description_table <- rbindlist(rs)
setnames(title_description_table, 1:3, c("Type", "Title", "Description"))

knitr::kable(title_description_table, caption = "Basic information about titles and descriptions")
```

```{r title_description_wordclouds, results='asis', echo=FALSE, message=FALSE}
generate_wordcloud_data <- function(texts) {

  #get words as a list of vectors
  word_list <- lapply(texts, function(x) unlist(strsplit(x, " ")))
  
  #remove duplicates
  word_list <- word_list[!duplicated(lapply(word_list, sort))]
  
  #clean up words
  docs <- Corpus(VectorSource(word_list))
  docs <- tm_map(docs, removeNumbers)
  docs <- tm_map(docs, removePunctuation)
  docs <- tm_map(docs, stripWhitespace)
  docs <- tm_map(docs, content_transformer(tolower))
  docs <- tm_map(docs, removeWords, stopwords("english"))
  
  #calculate dtm & freqs
  dtm <- TermDocumentMatrix(docs) 
  matrix <- as.matrix(dtm) 
  words <- sort(rowSums(matrix),decreasing=TRUE) 
  df <- data.frame(word = names(words),freq=words, stringsAsFactors = F)
  
  return(list(dtm = matrix, words = df))
}

#disable warnings
defaultW <- getOption("warn")
options(warn = -1)
old_par <- par()

words_titles <- generate_wordcloud_data(data_all[title!=""]$title)
words_desc <- generate_wordcloud_data(data_all[description!=""]$description)


if(save) {
  pdf(here::here("plots", "descriptions_wordcloud.pdf"), width=12.5, height=8)
}
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Descriptions word cloud")
descriptions_wordcloud_plot <- wordcloud(words = words_desc$words$word, 
                                         freq = words_desc$words$freq, min.freq = 3,
                                         max.words=100, random.order=FALSE, rot.per=0.35,
                                         colors=brewer.pal(8, "Dark2"))
if(save) {
  dev.off()
}

if(save == T) {
  pdf(here::here("plots", "titles_wordcloud.pdf"), width=12.5, height=8)
}
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Titles word cloud")
titles_wordcloud_plot <- wordcloud(words = words_titles$words$word, 
                                   freq = words_titles$words$freq, min.freq = 3,
                                   max.words=100, random.order=FALSE, rot.per=0.35,
                                   colors=brewer.pal(8, "Dark2"))

if(save == T) {
  dev.off()
}

#back to default
par(old_par) 
options(warn = defaultW)

```

```{r title_description_plots, results='asis', echo=FALSE, message=FALSE}
plot_count_title <- ggplot(data_vis["title"!=""], aes(y=`#Characters in title`, x=as.factor(position))) +
  geom_boxplot(color="white", fill="steelblue") +
  ylab("#Characters") +
  xlab("Position") +
  labs(title = "Title") +
  theme(plot.title = element_text(hjust = 0.5))
plot_count_description <-  ggplot(data_vis[description!=""], aes(y=`#Characters in description`, x=as.factor(position))) +
  geom_boxplot(color="white", fill="steelblue") +
  ylab("#Characters") +
  xlab("Position") +
  labs(title = "Description") +
  theme(plot.title = element_text(hjust = 0.5))
title_description_counts_plot <- grid.arrange(plot_count_title, plot_count_description, nrow=1, ncol=2, top="Non-empty title and description length boxplots")

plot_missing_description <- ggplot(data_vis[description==""], aes(x=position)) +
  geom_histogram(color="white", fill="steelblue", bins=20) +
  ylab("Count") +
  xlab("Position") + 
  labs(title = "Yes") +
  theme(plot.title = element_text(hjust = 0.5))
plot_not_missing_description <- ggplot(data_vis[description!=""], aes(x=position)) +
  geom_histogram(color="white", fill="steelblue", bins=20) +
  ylab("Count") +
  xlab("Position") + 
  labs(title = "No") +
  theme(plot.title = element_text(hjust = 0.5))
description_missing_plot <- grid.arrange(plot_missing_description, plot_not_missing_description, nrow=1, ncol=2, top="Empty description")


plot_car_personal_desc0 <- ggplot(data_vis[`Has car accident or personal injury in description` == 0], aes(x=position)) +
  geom_histogram(color="white", fill="steelblue", bins=20) +
  ylab("Count") +
  xlab("Position") + 
  labs(title = "No \"car accident\" or \"personal injury\"") +
  theme(plot.title = element_text(hjust = 0.5))
plot_car_personal_desc1 <- ggplot(data_vis[`Has car accident or personal injury in description` == 1], aes(x=position)) +
   geom_histogram(color="white", fill="steelblue", bins=20) +
   ylab("Count") +
   xlab("Position") + 
   labs(title = "Has \"car accident\" or \"personal injury\"") +
   theme(plot.title = element_text(hjust = 0.5))
plot_car_personal_title0 <- ggplot(data_vis[`Has car accident or personal injury in title` == 0], aes(x=position)) +
  geom_histogram(color="white", fill="steelblue", bins=20) +
  ylab("Count") +
  xlab("Position") + 
  labs(title = "No \"car accident\" or \"personal injury\"") +
  theme(plot.title = element_text(hjust = 0.5))
plot_car_personal_title1 <- ggplot(data_vis[`Has car accident or personal injury in title` == 1], aes(x=position)) +
  geom_histogram(color="white", fill="steelblue", bins=20) +
  ylab("Count") +
  xlab("Position") + 
  labs(title = "Has \"car accident\" or \"personal injury\"") +
  theme(plot.title = element_text(hjust = 0.5))

plot_lawyer_attorney_desc0 <- ggplot(data_vis[`Has lawyer or attorney in description` == 0], aes(x=position)) +
  geom_histogram(color="white", fill="steelblue", bins=20) +
  ylab("Count") +
  xlab("Position") + 
  labs(title = "No \"lawyer\" or \"attorney\"") +
  theme(plot.title = element_text(hjust = 0.5))
plot_lawyer_attorney_desc1 <- ggplot(data_vis[`Has lawyer or attorney in description` == 1], aes(x=position)) +
  geom_histogram(color="white", fill="steelblue", bins=20) +
  ylab("Count") +
  xlab("Position") + 
  labs(title = "Has \"lawyer\" or \"attorney\"") +
  theme(plot.title = element_text(hjust = 0.5))
plot_lawyer_attorney_title0 <- ggplot(data_vis[`Has lawyer or attorney in title` == 0], aes(x=position)) +
  geom_histogram(color="white", fill="steelblue", bins=20) +
  ylab("Count") +
  xlab("Position") + 
  labs(title = "No \"lawyer\" or \"attorney\"") +
  theme(plot.title = element_text(hjust = 0.5))
plot_lawyer_attorney_title1 <- ggplot(data_vis[`Has lawyer or attorney in title` == 1], aes(x=position)) +
  geom_histogram(color="white", fill="steelblue", bins=20) +
  ylab("Count") +
  xlab("Position") + 
  labs(title = "Has \"lawyer\" or \"attorney\"") +
  theme(plot.title = element_text(hjust = 0.5))

plot_city_desc0 <- ggplot(data_vis[`Has same city listed as in search query`==1], aes(x=position)) +
  geom_histogram(color="white", fill="steelblue", bins=20) +
  ylab("Count") +
  xlab("Position") + 
  labs(title = "No city name") +
  theme(plot.title = element_text(hjust = 0.5))
plot_city_desc1 <- ggplot(data_vis[`Has city in description` == 1], aes(x=position)) +
  geom_histogram(color="white", fill="steelblue", bins=20) +
  ylab("Count") +
  xlab("Position") + 
  labs(title = "Has city name") +
  theme(plot.title = element_text(hjust = 0.5))
plot_city_title0 <- ggplot(data_vis[`Has city in title` == 0], aes(x=position)) +
  geom_histogram(color="white", fill="steelblue", bins=20) +
  ylab("Count") +
  xlab("Position") + 
  labs(title = "No city name") +
  theme(plot.title = element_text(hjust = 0.5))
plot_city_title1 <- ggplot(data_vis[`Has city in title` == 1], aes(x=position)) +
  geom_histogram(color="white", fill="steelblue", bins=20) +
  ylab("Count") +
  xlab("Position") + 
  labs(title = "Has city name") +
  theme(plot.title = element_text(hjust = 0.5))

title_keywords_plot <- grid.arrange(plot_car_personal_title0, plot_car_personal_title1,
             plot_lawyer_attorney_title0, plot_lawyer_attorney_title1,
             plot_city_title0, plot_city_title1,
             nrow=3, ncol=2, top="Title containing keywords")

description_keywords_plot <- grid.arrange(plot_car_personal_desc0, plot_car_personal_desc1,
             plot_lawyer_attorney_desc0,  plot_lawyer_attorney_desc1,
             plot_city_desc0, plot_city_desc1,
             nrow=3, ncol=2, top="Description containing keywords")

if(save == T){ 
  ggsave(here::here("plots", "title_description_counts.pdf"), title_description_counts_plot, 
         width = 12.5, height = 8, device = cairo_pdf) 
  ggsave(here::here("plots", "description_missing.pdf"), description_missing_plot, 
         width = 12.5, height = 8, device = cairo_pdf) 
  ggsave(here::here("plots", "title_keywords.pdf"), title_keywords_plot, 
         width = 12.5, height = 8, device = cairo_pdf) 
  ggsave(here::here("plots", "description_keywords.pdf"), description_keywords_plot, 
         width = 12.5, height = 8, device = cairo_pdf)
  
}
```  

Key takeaways:

* In general, length of titles/descriptions don't correlate much with positions
* The only exception is businesses with missing description; they tend to have clearly lower positions in results
* In all cases with different keywords, search results containing keyword in title/description have lower positions on average than entries without
* The effect is more noticeable for titles than descriptions
* Also, more specific words (both city name and "car accident"/"personal injury") have higher effect then lawyer/attorney

### Reviews
```{r reviews_table, results='asis', echo=FALSE}

rs <- list()
rs[[length(rs)+1]] <- list("Median #reviews", 
                           median(data_all$reviews, na.rm=T))

rs[[length(rs)+1]] <- list("Max #reviews", 
                           max(data_all$reviews, na.rm=T))

rs[[length(rs)+1]] <- list("No reviews available", 
                           to_percentage(sum(is.na(data_all$rating)) / nrow(data_all)))

rs[[length(rs)+1]] <- list("Average rating", 
                           round(mean(data_all$rating, na.rm=T), digits=2))

rs[[length(rs)+1]] <- list("Response ratio by owners", 
                          to_percentage(sum(all_reviews$all_reviews_response_from_owner!="")/nrow(all_reviews)))

rs[[length(rs)+1]] <- list("Average number of likes per review", 
                           round(mean(all_reviews$all_reviews_number_of_likes, na.rm=T), digits=2))

rating_table <- rbindlist(rs)
setnames(rating_table, 1:2, c("Type", "Value"))

knitr::kable(rating_table, caption = "Basic information about reviews")
```  

```{r reviews_plots, results='asis', echo=FALSE}

rating_distribution_plot <- ggplot(all_reviews, aes(x=all_reviews_rating)) +
  geom_histogram(color="white", fill="steelblue", bins=20) +
  ylab("Count") +
  xlab("Rating") + 
  labs(title = "Ratings distribution") +
  theme(plot.title = element_text(hjust = 0.5)) + 
  coord_flip()
plot(rating_distribution_plot)

review_position_plot <- ggplot(data_vis, aes(y=`Relative #reviews`, x=as.factor(position))) +
  geom_boxplot(color="white", fill="steelblue") +
  ylab("Relative #reviews") +
  xlab("Position") +
  labs(title = "Relative #reviews boxplot") +
  theme(plot.title = element_text(hjust = 0.5))
plot(review_position_plot)

rating_position_plot <- ggplot(data_vis[!is.na(`Review rating`)], aes(y=`Review rating`, x=as.factor(position))) +
  geom_boxplot(color="white", fill="steelblue") +
  ylab("Review rating") +
  xlab("Position") +
  labs(title = "Review rating boxplot") +
  theme(plot.title = element_text(hjust = 0.5))
plot(rating_position_plot)

if(save == T){ 
  ggsave(here::here("plots", "rating_distribution.pdf"), rating_distribution_plot, 
         width = 12.5, height = 8, device = cairo_pdf) 
  ggsave(here::here("plots", "review_position.pdf"), review_position_plot, 
         width = 12.5, height = 8, device = cairo_pdf) 
  ggsave(here::here("plots", "rating_position.pdf"), rating_position_plot, 
         width = 12.5, height = 8, device = cairo_pdf) 
  
  description_missing_plot <- grid.arrange(plot_missing_description, plot_not_missing_description, nrow=1, ncol=2, top="Empty description")

}
```  

Key takeaways:

* Businesses with highest number of reviews tend to have the lowest positions (top left corner)
* In contrast, low number of reviews correlates with higher position (bottom right)
* Perhaps surprisingly, ratings themself don't seem to practically any effect on page's position, only activity matters. 
* But then on the other hand, if almost 90% of ratings are five stars and average rating is over 4.5, maybe there just isn't much room for a differentiation.


### Provides updates on Google

```{r provides_updates_plot, results='asis', echo=FALSE}
updates0 <- ggplot(data_vis[`Provides updates` == 0], aes(x=position)) +
    geom_histogram(color="white", fill="steelblue", bins=20) +
    ylab("Count") +
    xlab("Position") + 
    labs(title = "No") +
    theme(plot.title = element_text(hjust = 0.5))

updates1 <- ggplot(data_vis[`Provides updates` == 1], aes(x=position)) +
    geom_histogram(color="white", fill="steelblue", bins=20) +
    ylab("Count") +
    xlab("Position") + 
    labs(title = "Yes") +
    theme(plot.title = element_text(hjust = 0.5))

provides_updates_plot <- grid.arrange(updates1, updates0, nrow=1, ncol=2, top="Provides updates on Google")

if(save == T){ 
  ggsave(here::here("plots", "provides_updates.pdf"), provides_updates_plot, 
         width = 12.5, height = 8, device = cairo_pdf) 
}
``` 

### Backlink

```{r ref_domains_dofollow_table, results='asis', echo=FALSE}

rs <- list()
rs[[length(rs)+1]] <- list('Median', 
                           median(data_all$ref_domains_dofollow, na.rm=T),
                           median(data_all$total_traffic, na.rm=T),
                           median(data_all$ahrefs_rank, na.rm=T),
                           median(data_all$domain_rating, na.rm=T))

rs[[length(rs)+1]] <- list('Min', 
                           min(data_all$ref_domains_dofollow, na.rm=T),
                           min(data_all$total_traffic, na.rm=T),
                           min(data_all$ahrefs_rank, na.rm=T),
                           min(data_all$domain_rating, na.rm=T))

rs[[length(rs)+1]] <- list('Max', 
                           max(data_all$ref_domains_dofollow, na.rm=T),
                           max(data_all$total_traffic, na.rm=T),
                           max(data_all$ahrefs_rank, na.rm=T),
                           max(data_all$domain_rating, na.rm=T))

rs[[length(rs)+1]] <- list("Missing", 
                           to_percentage(sum(is.na(data_all$ref_domains_dofollow)) / nrow(all_reviews)),
                           to_percentage(sum(is.na(data_all$total_traffic)) / nrow(all_reviews)),
                           to_percentage(sum(is.na(data_all$ahrefs_rank)) / nrow(all_reviews)),
                           to_percentage(sum(is.na(data_all$domain_rating)) / nrow(all_reviews)))

backlink_table <- rbindlist(rs)
setnames(backlink_table, 1:5, c("Type", "ref_domains_dofollow", "total_traffic", "ahrefs_rank", "domain_rating"))

knitr::kable(backlink_table, caption = "Basic information about backlinks")

```

```{r ref_domains_dofollow_plots, results='asis', echo=FALSE}
ref_domains_dofollow_plot <- ggplot(data_vis[!is.na(`Relative ref_domains_dofollow`)],
       aes(y=`Relative ref_domains_dofollow`, x=as.factor(position))) +
  geom_boxplot(color="white", fill="steelblue") +
  ylab("Relative ref_domains_dofollow") +
  xlab("Position") +
  labs(title = "Relative ref_domains_dofollow boxplot") +
  theme(plot.title = element_text(hjust = 0.5))
plot(ref_domains_dofollow_plot)

total_traffic_plot <- ggplot(data_vis[!is.na(`Relative total_traffic`)],
       aes(y=`Relative total_traffic`, x=as.factor(position))) +
  geom_boxplot(color="white", fill="steelblue") +
  ylab("Relative total_traffic") +
  xlab("Position") +
  labs(title = "Relative total_traffic boxplot") +
  theme(plot.title = element_text(hjust = 0.5))
plot(total_traffic_plot)

ahrefs_rank_plot <- ggplot(data_vis[!is.na(`Relative ahrefs_rank`)],
       aes(y=`Relative ahrefs_rank`, x=as.factor(position))) +
  geom_boxplot(color="white", fill="steelblue") +
  ylab("Relative ahrefs_rank") +
  xlab("Position") +
  labs(title = "Relative ahrefs_rank boxplot") +
  theme(plot.title = element_text(hjust = 0.5))
plot(ahrefs_rank_plot)

domain_rating_plot <- ggplot(data_vis[!is.na(`Relative domain_rating`)],
       aes(y=`Relative domain_rating`, x=as.factor(position))) +
  geom_boxplot(color="white", fill="steelblue") +
  ylab("Relative domain_rating") +
  xlab("Position") +
  labs(title = "Relative domain_rating boxplot") +
  theme(plot.title = element_text(hjust = 0.5))
plot(domain_rating_plot)

if(save == T){ 
  ggsave(here::here("plots", "ref_domains_dofollow.pdf"), ref_domains_dofollow_plot, 
         width = 12.5, height = 8, device = cairo_pdf) 
  
  ggsave(here::here("plots", "total_traffic.pdf"), total_traffic_plot, 
         width = 12.5, height = 8, device = cairo_pdf)
    
  ggsave(here::here("plots", "ahrefs_rank.pdf"), ahrefs_rank_plot, 
         width = 12.5, height = 8, device = cairo_pdf)
  
  ggsave(here::here("plots", "domain_rating.pdf"), domain_rating_plot, 
         width = 12.5, height = 8, device = cairo_pdf)
}
```  

Key takeaways:

* Higher ref_domains_dofollow and total_traffic are more common for better positions
* ahrefs_rank seems to be telling the same story, the only difference is that small number for rank is better, so the shape is inverted

```{r convert_to_pngs, results='asis', echo=FALSE}
  if(save == T) {
    pdfs <- list.files(here::here("plots"), pattern = "*.pdf")
    
    if(!dir.exists(here::here("plots", "png"))) {
      dir.create(here::here("plots", "png"))
    }
    setwd(here::here("plots", "png"))

    for(pdf in pdfs) {
      pdf_convert(pdf = here::here("plots", pdf), format = "png", dpi = 720)
    }
  }

```  